{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "\n",
    "Welcome to the first classification programming assignment. You will now implement all that you have learnt about logistic regression and classification in this notebook. __Make sure to watch the short video with a few notes on the notebook before starting this exersize__, and good luck!\n",
    "\n",
    "\n",
    "*Use ctrl + on windows, or cmd + on mac to make the code bigger, and relpace the + with a - to make the code smaller*\n",
    "\n",
    "*Press __shift + enter__ to run a code block*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages ##\n",
    "\n",
    "In this first __cell__ you will import some packages that will be useful during the programming assignment\n",
    "\n",
    "+ __Numpy__ is used a lot for mathematical operations in python, and is especially helpful with matrices and vectors\n",
    "\n",
    "+ __Matplotlib__ is usedd for plotting graphs and showing vizualized data\n",
    "\n",
    "+ __PIL__ is used for image processing\n",
    "\n",
    "+ __Pandas__ is really helpful for reading and manipulating data (for example from .csv files)\n",
    "\n",
    "+ __os__ is just your operating system\n",
    "\n",
    "+ __glob__ is used for pathnames (ie file directories)\n",
    "\n",
    "+ __sklearn__ is a free software machine learning library for Python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "#setting a seed so that all the results will remain constant\n",
    "np.random.seed(3)\n",
    "\n",
    "#This just makes sure that the plots are shown in the Jupyter notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling ##\n",
    "\n",
    "Once you understandd how to create ML applications this is the part that usually takes the longest, but I will just explain what is happening here, and there is nothing in this part that you will need to do.\n",
    "\n",
    "I have commented in some code boxes that you can change some things to see different things, but do not change anything not specified\n",
    "\n",
    "Below is where the labels are loaded in, and so is the training and test images. The images are then converted into flattenedd vectors ready to be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Opening a .csv file full of labels for the data you are going to train on\n",
    "\n",
    "labels_temp = pd.read_csv('data/labels.csv')\n",
    "\n",
    "#showing the top of the data to make sure it has loaded properly\n",
    "\n",
    "labels_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the labels into a numpy array and printing its shape\n",
    "\n",
    "labels = np.array(labels_temp)\n",
    "\n",
    "labels = labels.T\n",
    "\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "You can go and change the image or image2 directories and try and see what other images are in the\n",
    "training data. Just a note - the dog images do not go from 0 to m, they are random numbers so you will\n",
    "have to go to the actual file you downloaded to see what the other filenames are.\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "#loading the image\n",
    "image = Image.open('data/dog images/11.png')\n",
    "image2 = Image.open('data/false images/12false.png')\n",
    "\n",
    "#converting into numpy array\n",
    "data = np.array(image)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "# summarize image details\n",
    "print(image2.mode)\n",
    "print(image2.size)\n",
    "\n",
    "\n",
    "''' change image2 to image below if you want to see that'''\n",
    "plt.imshow(image2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the images ###\n",
    "\n",
    "Below is where I load in all the images from the files. __You will have to enter your directories in place of mine, the same way I have done it__. I have marked with comments where you will have to change something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "\n",
    "This is the path /Users/AasmaanY/Desktop/AI teaching thing/testAlg/data. You need to implement the path to\n",
    "where your downloaded 'data' file is.\n",
    "\n",
    "'''\n",
    "\n",
    "''' CODE BELOW '''\n",
    "PATH = os.path.abspath(os.path.join('/Users/AasmaanY/Desktop/AI teaching thing/testAlg', 'data'))\n",
    "''' CODE ABOVE '''\n",
    "\n",
    "#You can leave this line as it is, it is just going to the dog images file (ie the file with labels 1)\n",
    "SOURCE_IMAGES = os.path.join(PATH, \"dog images\")\n",
    "\n",
    "# Just adding a .png to the end of the images to make sure they are the correct file type\n",
    "images = glob(os.path.join(SOURCE_IMAGES, \"*.png\"))\n",
    "\n",
    "# Size of data\n",
    "NUM_IMAGES = len(images)\n",
    "HEIGHT = 64\n",
    "WIDTH = 64\n",
    "CHANNELS = 3\n",
    "SHAPE = (HEIGHT, WIDTH, CHANNELS)\n",
    "\n",
    "#checking if algorithm is working\n",
    "print(\"This means the algorithm is working so far. Number of images = \" + str(NUM_IMAGES))\n",
    "\n",
    "\n",
    "for i,img in enumerate(images):\n",
    "    #'''enumerate takes all objects and lays them out 1 by 1 to loop through'''\n",
    "\n",
    "    #'''opening the image'''\n",
    "    imageTemp = Image.open(img)\n",
    "    \n",
    "    #'''converting from an image into a multi dimensional array dimensions - (height,width,channels)'''\n",
    "    image = np.asarray(imageTemp)\n",
    "\n",
    "    #'''reshaping the array into a (1,h*w*c) shaped matrix'''\n",
    "    image0 = image.reshape((1,HEIGHT*WIDTH*CHANNELS))\n",
    "\n",
    "    #'''below is stacking the row vectors on top of each other to make a (m,h*w*c) matrix'''\n",
    "    #'''where m is the number of images'''\n",
    "    \n",
    "    if i != 0:\n",
    "        imageFinalArray = np.vstack((imageFinalArray, image0))\n",
    "    else:\n",
    "        imageFinalArray = image0\n",
    "    \n",
    "    #print(imageFinalArray.shape)\n",
    "\n",
    "#'''-------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "#'''the bit above was for dog images, bit below is for non dog images'''\n",
    "\n",
    "#'''-------------------------------------------------------------------------------------------------------------'''\n",
    "\n",
    "#this is the path to the non dog images\n",
    "SOURCE_IMAGES = os.path.join(PATH, \"false images\")\n",
    "\n",
    "images = glob(os.path.join(SOURCE_IMAGES, \"*.png\"))\n",
    "\n",
    "#below is basically the same thing as above, but for the non dog images\n",
    "for i,img in enumerate(images):\n",
    "\n",
    "    imageTemp = Image.open(img)\n",
    "\n",
    "    image = np.asarray(imageTemp)\n",
    "\n",
    "    image0 = image.reshape((1,HEIGHT*WIDTH*CHANNELS))\n",
    "\n",
    "    imageFinalArray = np.vstack((imageFinalArray, image0))\n",
    "\n",
    "    \n",
    "# printing the final input array's shape\n",
    "\n",
    "print()\n",
    "print(\"Final input array's shape before transposing : \" + str(imageFinalArray.shape))\n",
    "print()\n",
    "print(\"Final input array's shape : \")\n",
    "\n",
    "'''transposing the array (swapping the dimensions), changing name to train input dataset'''\n",
    "'''dividing by 255 to normalize dataset (all values will be between 0 & 1)'''\n",
    "train_x = imageFinalArray.T\n",
    "print(train_x.shape)\n",
    "#print(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffling the data and the labels. You can uncomment the prints to see the shapes of the different varirables - which I think is really helpful to understand\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "x2 = train_x.T\n",
    "labels2 = labels.T\n",
    "\n",
    "#print(labels2.shape)\n",
    "#print(x2.shape)\n",
    "\n",
    "x_shuffled, y_shuffled = shuffle(x2, labels2, random_state=3)\n",
    "\n",
    "#print(x_shuffled.shape)\n",
    "#print(y_shuffled.shape)\n",
    "#print(y_shuffled[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here you can change the index to see different images in the TRAINING SET\n",
    "x_shuffledTESTING = x_shuffled.T\n",
    "y_shuffledTESTING = y_shuffled.T\n",
    "\n",
    "imgSize = 64\n",
    "\n",
    "'''CODE BELOW'''\n",
    "#index has to be between 0 and 307 because that is the number of images that are in the file\n",
    "index = 42\n",
    "'''CODE ABOVE'''\n",
    "plt.imshow(x_shuffledTESTING[:,index].reshape((imgSize, imgSize, 3)))\n",
    "\n",
    "print(\"label = \" + str(y_shuffledTESTING[:,index]) + \", image index = \" + str(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is just an example showing how the shuffle function works, and shuffels the labels and the data\n",
    "\n",
    "a = [ [[1,1],[10,10]] ,[[2,2],[20,20]] , [[3,3],[30,30]] ]\n",
    "b = [1,2,3]  \n",
    "\n",
    "a2, b2 = shuffle(a, b, random_state=3)\n",
    "\n",
    "print(a2)\n",
    "print(b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is splitting the data and labels into train and test sets\n",
    "\n",
    "X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(x_shuffled, y_shuffled, test_size=0.1, random_state = 3)\n",
    "\n",
    "#printing the shapes of the train and test sets\n",
    "print(X_train_temp.shape, X_test_temp.shape, y_train_temp.shape, y_test_temp.shape)\n",
    "\n",
    "#storing the size of the train and test set\n",
    "m_train = X_train_temp.shape[0]\n",
    "m_test = X_test_temp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing data\n",
    "train_set_x = X_train_temp.T/255.\n",
    "test_set_x = X_test_temp.T/255.\n",
    "\n",
    "#Transposing to get the correct shape\n",
    "train_set_y = y_train_temp.T\n",
    "test_set_y = y_test_temp.T\n",
    "\n",
    "#printing the different shapes\n",
    "print(\"Shape of the train set : \" + str(train_set_x.shape))\n",
    "print(\"Shape of the test set : \" + str(test_set_x.shape))\n",
    "print(\"Shape of the train labels : \" + str(train_set_y.shape))\n",
    "print(\"Shape of the test labels : \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid function##\n",
    "\n",
    "You will now make the sigmoid function which will compute the sigmoid of z:\n",
    "\n",
    "`Arguments:`\n",
    "\n",
    "`z -- A scalar or numpy array of any size.`\n",
    "\n",
    "`Return:`\n",
    "\n",
    "`s -- sigmoid(z)`\n",
    "\n",
    "\n",
    "___Using the notes you have made, fill in the code below___\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "$sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$\n",
    "\n",
    "Use `np.exp()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "\n",
    "    ### CODE BELOW\n",
    "    \n",
    "    s = None\n",
    "    \n",
    "    ### CODE ABOVE\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (\"sigmoid([-4, 9]) = \" + str(sigmoid(np.array([-4,9]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "sigmoid([-4, 9]) = [0.01798621 0.99987661]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing parameters\n",
    "\n",
    "In the code block below you will initialize the parameters (the weights and biases). You will *initialize the weights randomly, but __multiply the weights by 0.01__*. The bias can simply be initialized to zero\n",
    "\n",
    "\n",
    "`Arguments:`\n",
    "\n",
    "`dim -- size of the weight vector`\n",
    "\n",
    "`Return:`\n",
    "\n",
    "`w -- initialized vector of shape (dim, 1)`\n",
    "\n",
    "`b -- initialized scalar (corresponds to the bias)`\n",
    "\n",
    "\n",
    "___Using the notes you have made, fill in the code below___\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "Use `np.random.randn()`, look up its documentation online.\n",
    "\n",
    "*Hint 2:*\n",
    "\n",
    "It takes the dimensions as arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_random(dim):\n",
    "\n",
    "    np.random.seed(3)\n",
    "    \n",
    "    ### CODE BELOW\n",
    "    \n",
    "    w = None\n",
    "    \n",
    "    b = None\n",
    "    \n",
    "    ### CODE ABOVE\n",
    "\n",
    "    assert(w.shape == (dim, 1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 3\n",
    "w, b = initialize_random(dim)\n",
    "print (\"w = \" + str(w))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "w = [[0.01788628]\n",
    " [0.0043651 ]\n",
    " [0.00096497]]\n",
    " \n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Computing predictions, cost, and derivatives\n",
    "\n",
    "In the code block below you will compute A, the cost, \"dw\" and \"db\". *For this part you will need to refer to any notes you have made, or to the video about the vectorized implementation of logistic regression*. This will set up the parameters ready for gradient descent\n",
    "\n",
    "\n",
    "`Arguments:`\n",
    "\n",
    "`w -- weights, a vector (num_px * num_px * 3, 1)`\n",
    "\n",
    "`b -- bias, a scalar`\n",
    "\n",
    "`X -- data, a matrix (num_px * num_px * 3, number of examples)`\n",
    "\n",
    "`Y -- label, vector (1, number of examples)`\n",
    "\n",
    "`Return:`\n",
    "\n",
    "`cost -- negative log-likelihood cost for logistic regression`\n",
    "\n",
    "`dw -- gradient of the cost with respect to w`\n",
    "\n",
    "`db -- gradient of the cost with respect to b`\n",
    "\n",
    "\n",
    "___Using the notes you have made, fill in the code below___\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "Look at vectorized implementation video\n",
    "\n",
    "*Hint 2:*\n",
    "\n",
    "Use `np.log()` for logs,  `np.dot()` for matrix multiplication (NOT element-wise multiplication), `np.sum()` for summations, `*` for element-wise multiplications, and the `sigmoid()` function you implemented for the sigmoid function\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grads_and_cost(w, b, X, Y):\n",
    "    \n",
    "    \n",
    "    #number of examples in input\n",
    "    m = X.shape[1]\n",
    "    \n",
    "\n",
    "    # calculating prediction and cost\n",
    "    \n",
    "    ### CODE BELOW\n",
    "    \n",
    "    #calculating the prediction\n",
    "    \n",
    "    Z = None\n",
    "    \n",
    "    A = None\n",
    "    \n",
    "    # compute cost DONT FORGET np.sum()\n",
    "    cost = None\n",
    "    \n",
    "    # calculating derivatives\n",
    "\n",
    "    \n",
    "    #calculating (dcost/dw) = \"dw\"\n",
    "    dw = None\n",
    "    \n",
    "    #calculating (dcost/db) = \"db\"\n",
    "    db = None\n",
    "    \n",
    "    ### CODE ABOVE\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    gradients = {\"dw\": dw,\n",
    "                 \"db\": db}\n",
    "    \n",
    "    return gradients, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b, X, Y = np.array([[1],[1]]), 1, np.array([[3,-3,3],[3,-3,-3.3]]), np.array([[1,0,0]])\n",
    "gradients, cost = compute_grads_and_cost(w, b, X, Y)\n",
    "print (\"dw = \" + str(gradients[\"dw\"]))\n",
    "print (\"db = \" + str(gradients[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "dw = [[ 0.66058387][-0.74261045]]\n",
    "\n",
    "db = 0.2246565239660168\n",
    "\n",
    "cost = 0.3702709546094501"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the algorithm\n",
    "\n",
    "In this next function you will implement one step of gradient descent in order to make your algorithm learn, this will be called multiple times in a for loop in order to train the algorithm multiple times.\n",
    "\n",
    "`Arguments:`\n",
    "\n",
    "`w -- weights, a vector (num_px * num_px * 3, 1)`\n",
    "\n",
    "`b -- bias, a scalar`\n",
    "\n",
    "`X -- data, a matrix (num_px * num_px * 3, number of examples)`\n",
    "\n",
    "`Y -- label, vector (1, number of examples)`\n",
    "\n",
    "`num_iterations -- number of steps of gradient descent being implemented`\n",
    "\n",
    "`learning_rate -- the learning rate (alpha)`\n",
    "\n",
    "`Return:`\n",
    "\n",
    "`parameters -- dictionary containing the weights w and bias b`\n",
    "\n",
    "`gradients -- dictionary containing the gradients of the weights and bias`\n",
    "\n",
    "`costs -- list of all the costs computed during the optimization`\n",
    "\n",
    "\n",
    "\n",
    "___Using the notes you have made, fill in the code below___\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "Use previous functions you have made to calculate the costs and gradients and use gradient descent to update the weights and biases\n",
    "\n",
    "*Hint 2:*\n",
    "\n",
    "parameter = parameter - alpha * dparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(w, b, X, Y, num_iterations, learning_rate):\n",
    "\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        ### CODE BELOW\n",
    "        gradients, cost = None\n",
    "        ### CODE ABOVE\n",
    "        \n",
    "        # getting gradients that were computed\n",
    "        dw = gradients[\"dw\"]\n",
    "        db = gradients[\"db\"]\n",
    "        \n",
    "        # updadting the parameters\n",
    "        \n",
    "        ### CODE BELOW\n",
    "        w = None\n",
    "        b = None\n",
    "        ### CODE ABOVE\n",
    "        \n",
    "        # Recording and printing the costs every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    parameters = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    gradents = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return parameters, gradients, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, gradients, costs = train(w, b, X, Y, num_iterations= 100, learning_rate = 0.009)\n",
    "\n",
    "print (\"w = \" + str(parameters[\"w\"]))\n",
    "print (\"b = \" + str(parameters[\"b\"]))\n",
    "print (\"dw = \" + str(gradients[\"dw\"]))\n",
    "print (\"db = \" + str(gradients[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "Cost after iteration 0: 5.801545\n",
    "\n",
    "w = [[0.19033591][0.12259159]]\n",
    "\n",
    "b = 1.9253598300845747\n",
    "\n",
    "dw = [[0.67752042][1.41625495]]\n",
    "\n",
    "db = 0.21919450454067657\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training the algorithm\n",
    "\n",
    "Now that you have implemented code to 'learn' w and b, you can use the parameters to predict the (hopefully) correct labels for a dataset (of images)\n",
    "\n",
    "`Arguments:`\n",
    "\n",
    "`w -- weights, a vector (num_px * num_px * 3, 1)`\n",
    "\n",
    "`b -- bias, a scalar`\n",
    "\n",
    "`X -- data, a matrix (num_px * num_px * 3, number of examples)`\n",
    "\n",
    "`Return:`\n",
    "\n",
    "` Y_prediction -- a vector containing all predictions (0 or 1) for the examples in X`\n",
    "\n",
    "\n",
    "\n",
    "___Using the notes you have made, fill in the code below___\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "Use if statements to go from values between 1 and 0 to 1 or 0. Remember the prediction is equal to 1 if A >= 0.5\n",
    "--update this is done for you, because many people found this confusing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # compute the raw prediction value\n",
    "    \n",
    "    ### CODE BELOW\n",
    "    A = None\n",
    "    ### CODE ABOVE\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        # Convert probabilities ( A[0,i] ) to actual predictions ( p[0,i] )\n",
    "        \n",
    "        if A[0,i] >= 0.5:\n",
    "            Y_prediction[0,i] = 1\n",
    "        elif A[0,i] < 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0.333],[0.667]])\n",
    "b = 1\n",
    "X = np.array([[1.,-1,-3,7],[1,-7,0,-5]])\n",
    "print (\"predictions = \" + str(predict(w, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "predictions = [[1. 0. 1. 0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Now you will put everything together (in the correct order into a final model\n",
    "\n",
    "`Arguments:`\n",
    "\n",
    "`X_train -- training set, a matrix (num_px * num_px * 3, m_train)`\n",
    "\n",
    "`Y_train -- training labels, a row vector (1, m_train)`\n",
    "\n",
    "`X_test -- test set, a matrix (num_px * num_px * 3, m_test)`\n",
    "\n",
    "`Y_test -- test labels, a row vector (1, m_test)`\n",
    "\n",
    "`num_iterations -- number of steps of gradient descent being implemented`\n",
    "\n",
    "`learning_rate -- the learning rate (alpha)`\n",
    "\n",
    "`Return:`\n",
    "\n",
    "`d -- dictionary containing information about the model`\n",
    "\n",
    "\n",
    "\n",
    "___Using the notes you have made, fill in the code below___\n",
    "\n",
    "*Hint:*\n",
    "\n",
    "Put everything in order, as stated with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5):\n",
    "\n",
    "    ### CODE BELOW\n",
    "    \n",
    "    # initialize parameters with zeros\n",
    "    w, b = None\n",
    "    #has to be the same as the down length of matrix X ie X_train.shape[0]\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, gradients, costs = None\n",
    "\n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = None\n",
    "    b = None\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    Y_prediction_test = None\n",
    "    Y_prediction_train = None\n",
    "    \n",
    "    \n",
    "    ### CODE ABOVE\n",
    "    \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 300, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model I\n",
    "\n",
    "As you can see the accuracy was around 81% on the train set, but the accuracy on the test set was much lower (~71%). This is because of the hyperparameter selection. If you take bigger gradient descent steps (ie a higher learning rate, theoretically the algorithm should get a lower cost quicker, so let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 300, learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model II\n",
    "\n",
    "That wasn't good. Basically what happened here is that the steps were too big, and so the algorithm, instead of converging, actually chose more and more 'wrong' hyperparameters. Right now the algorithm is as good as guessing. Now let's try increasing the number of iterations instead of the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = 0.005)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model III\n",
    "\n",
    "Now that is looking much better! You can see the cost starts going down less and less and as you can see it is approaching the global minimum. Now I want you to try some hyperparameters, and if your hyperparameters do better than the model above, you can get a free virtual high five!(I did not spend a long time tuning it so I'm sure you can get better models easily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = None, learning_rate = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking what you classified wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a picture that was wrongly classified.\n",
    "\n",
    "num_px = 64\n",
    "\n",
    "falsePrediction = []\n",
    "\n",
    "for i in range(0,m_test-1):\n",
    "    if test_set_y[0,i] != d[\"Y_prediction_test\"][0,i]:\n",
    "        falsePrediction.append(i)\n",
    "\n",
    "print(\"Change the index to a number between 0 and \" + str(len(falsePrediction)-1) + \" to see images that you wrongly classified\" )\n",
    "index = falsePrediction[0]\n",
    "\n",
    "plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)))\n",
    "\n",
    "print (\"The actual label = \" + str(test_set_y[0,index]) + \", you predicted that y = \" + str(d[\"Y_prediction_test\"][0,index]) + \", which is wrong\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the cost function and the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot learning curve (with costs)\n",
    "costs = np.squeeze(d['costs'])\n",
    "plt.plot(costs)\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per hundreds)')\n",
    "plt.title(\"Learning rate =\" + str(d[\"learning_rate\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with your own image\n",
    "\n",
    "You can now use your own image and see the output of your model! To do that:\n",
    "    1. Find a .png or .jpg image that you want to test\n",
    "    2. Put it in the 'uploadedimages' file that you downloaded\n",
    "    3. Change the image path below to that image's filename\n",
    "    \n",
    "I have already uploaded an image that you can see the algorithm working on. It does classify correctly when the model is trained for 1500 iterations at a learning rate of 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''CODE BELOW'''\n",
    "\n",
    "#upload your image to the uploaded images file, and change the directory to the image you want to test on\n",
    "#make sure the image is a .png or .jpg, others file types may not work.\n",
    "img_path = 'uploadedimages/my_image.png'\n",
    "\n",
    "'''CODE ABOVE'''\n",
    "\n",
    "img_not_cropped = image.load_img(img_path)\n",
    "img = image.load_img(img_path, target_size=(64, 64))\n",
    "x = image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "x = x/255.0\n",
    "\n",
    "x_final = x.reshape((1, num_px*num_px*3)).T\n",
    "\n",
    "#print(x_final.shape)\n",
    "\n",
    "my_predicted_image = predict(d[\"w\"], d[\"b\"], x_final)\n",
    "\n",
    "if np.squeeze(my_predicted_image) == 1:\n",
    "    print(\"You predicted a \" + str(np.squeeze(my_predicted_image)) + \" which means the algorithm thinks the image is of a dog\")\n",
    "else:\n",
    "    print(\"You predicted a \" + str(np.squeeze(my_predicted_image)) + \" which means the algorithm thinks the image is not of a dog\")\n",
    "    \n",
    "plt.imshow(img_not_cropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please do go ahead and message me (Aasmaan) if you want any extra help or have any questions (not necessarily about this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "XaIWT",
   "launcher_item_id": "zAgPl"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
